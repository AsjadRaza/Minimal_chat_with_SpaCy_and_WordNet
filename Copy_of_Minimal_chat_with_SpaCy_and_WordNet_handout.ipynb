{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "Copy_of_Minimal_chat_with_SpaCy_and_WordNet_handout.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpbIC5q0-Q4U"
      },
      "source": [
        "# Building basic chatbots with rules, syntax and semantic nets\n",
        "\n",
        "It is increasingly often, that companies would like to automate internal or customer facing tasks via a chat interface. Though there are mature frameworks (like [RASA](https://rasa.com/)) or services (like [Microsoft Bot Framework](https://dev.botframework.com/) or [Chatfuel](https://chatfuel.com/)), we will attempt to set up a basic analysis pipeline based on SpaCy and WordNet, that can give us some coverage in a basic banking scenario.   \n",
        "\n",
        "We will use SpaCy for our basic analysis (including syntax), as well as a simple addon, that connects it to WordNet called unsurprisingly [SpaCy-WordNet](https://spacy.io/universe/project/spacy-wordnet).\n",
        "\n",
        "Let's take the following texts as a problem:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-11T11:45:51.229539Z",
          "start_time": "2019-11-11T11:45:51.225013Z"
        },
        "id": "y93oTZn1-Q4Z"
      },
      "source": [
        "test_texts = [\n",
        "    \"I would like to deposit 5000 euros.\",\n",
        "    \"I would like to put in 5000 euros.\",\n",
        "    \"I would like to pay in 5000 euros.\",\n",
        "    \"I would like to pay up 5000EUR.\",\n",
        "    \"Can I pay in 5000 euros, please?\",\n",
        "    \n",
        "    \n",
        "    \"I would like to deposit money.\",\n",
        "    \n",
        "\n",
        "    \"I am about to take out 5000 euros.\",\n",
        "    \"I am about to get out 5000 euros.\",\n",
        "    \"I am about to withdraw 5000 euros.\",\n",
        "    \"I want to withdraw 5000 USD.\",\n",
        "    \"Can I withdraw $5000.\",\n",
        "\n",
        "    \n",
        "    \"Can I check my account, please?\",\n",
        "    \"May I see my balance, please?\",\n",
        "    \"Could I query my account, please?\",\n",
        "    \"I would like to see my account balance.\"\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMXdyEHV-Q4h"
      },
      "source": [
        "## Let's try some syntactic analysis!\n",
        "\n",
        "The first goal is to see, if we can filter out, based on some common POS / Dependency structure the \"main message\", the things that people would like to say with the sentences above.\n",
        "\n",
        "The expected output based on our analysis would be something like:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQLSi4rO-Q4i"
      },
      "source": [
        "```\n",
        "[deposit, 5000, euros]\n",
        "[put, in, 5000, euros]\n",
        "[pay, in, 5000, euros]\n",
        "[pay, up, 5000EUR]\n",
        "[pay, in, 5000, euros]\n",
        "[deposit, money]\n",
        "[take, out, 5000, euros]\n",
        "----- No success in parsing. Original: I am about to get out 5000 euros.\n",
        "[withdraw, 5000, euros]\n",
        "[withdraw, 5000, USD]\n",
        "[withdraw, $, 5000]\n",
        "[check, my, account]\n",
        "[see, my, balance]\n",
        "[query, my, account]\n",
        "[see, my, account, balance]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r03Rlfbm-Q4i"
      },
      "source": [
        "%%capture\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUNpC6r3-Q4m"
      },
      "source": [
        "### Preliminaries: install SpaCy and initialize model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-11T11:45:55.737042Z",
          "start_time": "2019-11-11T11:45:54.894640Z"
        },
        "id": "e3WI6LQM-Q4q"
      },
      "source": [
        "import spacy\n",
        "import en_core_web_sm\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-11T11:45:55.759307Z",
          "start_time": "2019-11-11T11:45:55.738655Z"
        },
        "id": "Xo3G5Sai-Q4y"
      },
      "source": [
        "# We create one document out of the array of sentences for convenience.\n",
        "long_text = \" \".join(test_texts)\n",
        "\n",
        "doc = nlp(long_text ) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVylQg0JjnRe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "7b3dbdab-ab3e-4ffd-94d0-f4b0d5d80fca"
      },
      "source": [
        "long_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I would like to deposit 5000 euros. I would like to put in 5000 euros. I would like to pay in 5000 euros. I would like to pay up 5000EUR. Can I pay in 5000 euros, please? I would like to deposit money. I am about to take out 5000 euros. I am about to get out 5000 euros. I am about to withdraw 5000 euros. I want to withdraw 5000 USD. Can I withdraw $5000. Can I check my account, please? May I see my balance, please? Could I query my account, please? I would like to see my account balance.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmyruuKW-Q41"
      },
      "source": [
        "### Try some syntactic matching on the texts!\n",
        "\n",
        "Let us use the syntactic analysis of SpaCy to get to the \"core\" of the sentences!\n",
        "\n",
        "Let us assume, that we are interested in **verbs** and their **minimal subtrees**!\n",
        "\n",
        "Please\n",
        "\n",
        "1. look for the verbs in the sentences, \n",
        "2. get their subtrees,\n",
        "3. delete every token from the \"left\" of the verb\n",
        "4. from the \"right\" subtree, filter interjections and punctuations,\n",
        "5. keep the shortest such subtree from the sentence and print it out!\n",
        "\n",
        "For the visualization of the sentence tree use [DisplaCy](https://spacy.io/usage/visualizers)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-11T11:45:55.767176Z",
          "start_time": "2019-11-11T11:45:55.760799Z"
        },
        "id": "OFN2_0Eu-Q42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c3878d1-446f-49b7-b7b6-1972be0a9a95"
      },
      "source": [
        "\n",
        "for sentence in doc.sents:\n",
        "    sentence_subtree = []\n",
        "    \n",
        "    for token in sentence:\n",
        "      # proceed only if the token is a verb    \n",
        "      if token.pos_==\"VERB\":\n",
        "        # get the subtree for this token filtering unwanted POS tags, store in span\n",
        "        span = [token.text for token in token.subtree if token.pos_ not in (\"PUNCT\",  \"INTJ\")]\n",
        "        # get the left subtree for this token\n",
        "        lefts = [token.text for token in token.lefts]\n",
        "        # subtract left subtree form span\n",
        "        sentence_subtree = [token for token in span if token not in lefts]   \n",
        "   \n",
        "    # print after passing through the entire sentence\n",
        "    if sentence_subtree:\n",
        "      print([', '.join(w for w in sentence_subtree)])\n",
        "    else:\n",
        "      print(\"----- No success in parsing. Original:\",sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['deposit, 5000, euros']\n",
            "['put, in, 5000, euros']\n",
            "['pay, in, 5000, euros']\n",
            "['pay, up, 5000EUR']\n",
            "['pay, in, 5000, euros']\n",
            "['deposit, money']\n",
            "['take, out, 5000, euros']\n",
            "----- No success in parsing. Original: I am about to get out 5000 euros.\n",
            "['withdraw, 5000, euros']\n",
            "['withdraw, 5000, USD']\n",
            "['withdraw, $, 5000']\n",
            "['check, my, account']\n",
            "['see, my, balance']\n",
            "['query, my, account']\n",
            "['see, my, account, balance']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhFH_4RV-Q46"
      },
      "source": [
        "As we can see, even in this simple case, some noise remains, that is: with our method we can not achieve success by sentence 8. Please observe, and let's discuss, why!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-11T11:45:55.786989Z",
          "start_time": "2019-11-11T11:45:55.769377Z"
        },
        "id": "MbFtIfFl-Q47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "outputId": "9353b4bf-bcae-471b-d306-a356597670e3"
      },
      "source": [
        "from spacy import displacy\n",
        "\n",
        "doc=nlp(test_texts[7])\n",
        "\n",
        "displacy.render(doc, style=\"dep\", jupyter=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"4fe32d1a27b7467ab721b3f44b8e2b64-0\" class=\"displacy\" width=\"1450\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">am</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">about</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">to</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">PART</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">get</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">out</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADV</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">5000</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NUM</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">euros.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4fe32d1a27b7467ab721b3f44b8e2b64-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4fe32d1a27b7467ab721b3f44b8e2b64-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4fe32d1a27b7467ab721b3f44b8e2b64-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4fe32d1a27b7467ab721b3f44b8e2b64-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M390.0,266.5 L398.0,254.5 382.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4fe32d1a27b7467ab721b3f44b8e2b64-0-2\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4fe32d1a27b7467ab721b3f44b8e2b64-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M595,266.5 L587,254.5 603,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4fe32d1a27b7467ab721b3f44b8e2b64-0-3\" stroke-width=\"2px\" d=\"M420,264.5 C420,89.5 745.0,89.5 745.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4fe32d1a27b7467ab721b3f44b8e2b64-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M745.0,266.5 L753.0,254.5 737.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4fe32d1a27b7467ab721b3f44b8e2b64-0-4\" stroke-width=\"2px\" d=\"M770,264.5 C770,177.0 915.0,177.0 915.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4fe32d1a27b7467ab721b3f44b8e2b64-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prt</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M915.0,266.5 L923.0,254.5 907.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4fe32d1a27b7467ab721b3f44b8e2b64-0-5\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,177.0 1265.0,177.0 1265.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4fe32d1a27b7467ab721b3f44b8e2b64-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1120,266.5 L1112,254.5 1128,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4fe32d1a27b7467ab721b3f44b8e2b64-0-6\" stroke-width=\"2px\" d=\"M770,264.5 C770,2.0 1275.0,2.0 1275.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4fe32d1a27b7467ab721b3f44b8e2b64-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1275.0,266.5 L1283.0,254.5 1267.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBfBBppQ4evX"
      },
      "source": [
        "Answer: There is no individual full VERB in this sentence that's why we are getting this problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhAxFwQc-Q5D"
      },
      "source": [
        "It is worth noting, that some addon libraries, like [Textacy](https://spacy.io/universe/project/textacy) have built in functions that can come in handy in these topics.\n",
        "\n",
        "Like:\n",
        "\n",
        "`textacy.spacier.utils.get_main_verbs_of_sent(sent)`\n",
        "Return the main (non-auxiliary) verbs in a sentence.\n",
        "\n",
        "`textacy.spacier.utils.get_subjects_of_verb(verb)`\n",
        "Return all subjects of a verb according to the dependency parse.\n",
        "\n",
        "`textacy.spacier.utils.get_objects_of_verb(verb)`\n",
        "Return all objects of a verb according to the dependency parse, including open clausal complements.\n",
        "\n",
        "`textacy.spacier.utils.get_span_for_compound_noun(noun)`\n",
        "Return document indexes spanning all (adjacent) tokens in a compound noun.\n",
        "\n",
        "`textacy.spacier.utils.get_span_for_verb_auxiliaries(verb)`\n",
        "Return document indexes spanning all (adjacent) tokens around a verb that are auxiliary verbs or negations.\n",
        "\n",
        "None the less, if we want to carry out some definite actions for these sentences, we have to try another route."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i1P-8gW-Q5E"
      },
      "source": [
        "## Second try: detecting \"intents\" and \"entities\" with the help of WordNet\n",
        "\n",
        "In processing chat utterances, the two common tasks are to:\n",
        "\n",
        "1. Detect the overall intent of the given utterance\n",
        "2. Extract some key parameters needed for action.\n",
        "\n",
        "The first is called **\"intent detection\"** the second **\"entity extraction\"**.\n",
        "\n",
        "More on this can be found in the Theory section on chatbots, discussed later.\n",
        "\n",
        "Though the standard practice for the first step is to build up a sentence classifier, and the second is done usually with some token level classifier / matching, now we will utilize the same rule based matching mechanism of SpaCy that we did before, albeit with a twist.\n",
        "\n",
        "One of the main problems, as we saw before is the **variety of utterances**, that is, people tend to formulate the same intent in myriad ways. We will intend to mitigate this by **increasing coverage with WorNet synonyms**.\n",
        "\n",
        "For this we need a connection between our analysis pipeline and WordNet. Luckily, we have it as an extension.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXWndc_8-Q5E"
      },
      "source": [
        "### Install extension and register it to the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD7muMz-k7Wa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "outputId": "b1c1ee56-adac-422d-f6b3-3daba5f1a671"
      },
      "source": [
        "!pip install spacy-wordnet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacy-wordnet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/f2/4d8070df0f7a7a9eeed74eb7e9ce3cf41349eb5e06b1e088de9eeca630e2/spacy-wordnet-0.0.4.tar.gz (648kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 5.8MB/s \n",
            "\u001b[?25hCollecting nltk<3.4,>=3.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/09/3b1755d528ad9156ee7243d52aa5cd2b809ef053a0f31b53d92853dd653a/nltk-3.3.0.zip (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 23.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk<3.4,>=3.3->spacy-wordnet) (1.15.0)\n",
            "Building wheels for collected packages: spacy-wordnet, nltk\n",
            "  Building wheel for spacy-wordnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spacy-wordnet: filename=spacy_wordnet-0.0.4-py2.py3-none-any.whl size=650293 sha256=589380c35e7e7ebdd14071061af3fb8d796469bc0b6d0d8216e11d957cc97ea3\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/93/1d/c86db913cd146fc9ddb26d10f56579c5d58a3e00bc8f96a3a6\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-cp36-none-any.whl size=1394469 sha256=20333694910d308138bb788ee57ce03226ff0501d6020521e84d86fa2b64f063\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/ab/40/3bceea46922767e42986aef7606a600538ca80de6062dc266c\n",
            "Successfully built spacy-wordnet nltk\n",
            "Installing collected packages: nltk, spacy-wordnet\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.3 spacy-wordnet-0.0.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nltk"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-11T11:45:57.312343Z",
          "start_time": "2019-11-11T11:45:56.822682Z"
        },
        "id": "qy13RQOe-Q5H"
      },
      "source": [
        "from spacy_wordnet.wordnet_annotator import WordnetAnnotator \n",
        "\n",
        "nlp.add_pipe(WordnetAnnotator(nlp.lang), after='tagger') #Register to the pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnENjtDS-Q5K"
      },
      "source": [
        "### Setting up a custom detector for intents\n",
        "\n",
        "As said, we will hijack the entity detector capability of SpaCy to classify intents.\n",
        "\n",
        "For this, we need to define custom rules with `EntityRuler`, and some patterns that match our intents.\n",
        "\n",
        "We have all in all 3 intents in mind:\n",
        "\n",
        "`INTENTS = [\"TAKEOUT_INTENT\",\"PAYIN_INTENT\",\"BALANCE_INTENT\"]`\n",
        "\n",
        "First define patterns **one for each**, register it, try to run the pipeline, and see the result.\n",
        "\n",
        "After it, you will have to **get back to this cell and iteratively refine the pattern** based on the results of WordNet enrichment below.\n",
        "\n",
        "First make it run through, then refine!\n",
        "All in all 7 patterns are enough in total to detect the three intents in all their forms seen here with the help of WordNet synsets.\n",
        "\n",
        "#### Set up EntityRuler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-11T11:45:58.608620Z",
          "start_time": "2019-11-11T11:45:57.315205Z"
        },
        "id": "isZzRWcR-Q5L"
      },
      "source": [
        "from spacy.pipeline import EntityRuler\n",
        "\n",
        "INTENTS = [\"TAKEOUT_INTENT\",\"PAYIN_INTENT\",\"BALANCE_INTENT\"]\n",
        "\n",
        "ruler = EntityRuler(nlp, overwrite_ents=True)\n",
        "\n",
        "# by printing the sysnonyms of words in each sentence(later in this notebook)\n",
        "# i came up with these patterns to capture the intents. \n",
        "\n",
        "patterns = [{\"label\": INTENTS[0], \"pattern\": 'out'}, \n",
        "            {\"label\": INTENTS[1], \"pattern\": 'pay'},\n",
        "            {\"label\": INTENTS[1], \"pattern\": 'put'},\n",
        "            {\"label\": INTENTS[1], \"pattern\": 'deposit'},\n",
        "            {\"label\": INTENTS[2], \"pattern\": \"balance\"},\n",
        "            {\"label\": INTENTS[2], \"pattern\": \"query\"},\n",
        "            {\"label\": INTENTS[2], \"pattern\": \"check\"}]\n",
        "\n",
        "# Add the patterns to the ruler\n",
        "ruler.add_patterns(patterns)\n",
        "# Add the ruler to the pipeline\n",
        "nlp.add_pipe(ruler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUffoR-h-Q5Q"
      },
      "source": [
        "#### Define a `detect_intent` function\n",
        "\n",
        "The function takes in as an input an analysed sentence (`Doc`), a list on intents (eg. `INTENTS`), ad gives back the found intent or `None`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-11T11:45:58.613673Z",
          "start_time": "2019-11-11T11:45:58.610722Z"
        },
        "id": "3ef_VstS-Q5S"
      },
      "source": [
        "def detect_intent(analysed_sentence, intents):\n",
        "    # In this case, we do not do proper intent detection,\n",
        "    # which would be a whole sentence classification task, based on it's semantics,\n",
        "    # but we do an intelligent entity matching based on our rules,\n",
        "    # where we treat intents as special entities.\n",
        "    found=None\n",
        "    for ent in analysed_sentence.ents:\n",
        "      if ent.label_ in intents:\n",
        "        found = ent.label_\n",
        "    \n",
        "    return found"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UEmIwrw-Q5V"
      },
      "source": [
        "### Setitng up a function for detecting \"real\" entities\n",
        "\n",
        "In SpaCy's world, monetary units and numbers are considered to be entities by default, thus the built in Named Entity Recognizer (`ner` in the pipeline) detects and tags those.\n",
        "\n",
        "In our case we are only interested in the monetary entities. **Please bear in mind that MULTIPLE categories can mean money, so some times normal numbers, sometimes formal money, etc. Use multiple numeric categories for detection!**\n",
        "\n",
        "More on this [here](https://spacy.io/usage/linguistic-features/#named-entities) and [here](https://spacy.io/api/annotation#named-entities)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-11T11:45:58.622131Z",
          "start_time": "2019-11-11T11:45:58.614805Z"
        },
        "id": "LVkURYAA-Q5V"
      },
      "source": [
        "MONEY = [\"MONEY\", \"CARDINAL\", \"QUANTITY\"]\n",
        "\n",
        "def detect_money(analysed_sentence, money):\n",
        "    \n",
        "    found_money = None\n",
        "    only_numbers = ''\n",
        "    \n",
        "    for ent in analysed_sentence.ents:\n",
        "      if ent.label_ in MONEY:\n",
        "        found_money = ent.text   \n",
        "        \n",
        "    #if query is regarding balance intent, pass\n",
        "    if found_money == None:\n",
        "      pass   \n",
        "      \n",
        "    else:\n",
        "      #get the numbers only\n",
        "      for char in found_money:\n",
        "        if char.isdigit():\n",
        "          only_numbers+=char\n",
        "\n",
        "    # Please return only the numbers from the money!!!\n",
        "    \n",
        "    return only_numbers "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo45oZjt-Q5Y"
      },
      "source": [
        "### Enriching intent detection with WordNet\n",
        "\n",
        "As we well saw, if we don't want to manually set up the patterns that match all test cases - which is unsustainable for a much bigger corpus than this - we need some semantic help.\n",
        "\n",
        "Let's define a super crude `enrich_sentence` function, that generates sentence variants from the input. It takes in an analysed sentence (`doc`), a set of domains (in our case eg. `ECONOMY_DOMAINS`), and **for each token in the sentence searches for the sysnonyms inside our domains, then replaces the token with it's synonym, and appends the new sentence to a list.**\n",
        "\n",
        "**Finally we expect to get back a set of sentence variants as texts in a list.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-11T11:45:58.626815Z",
          "start_time": "2019-11-11T11:45:58.623370Z"
        },
        "id": "5qIor8rJ-Q5Y"
      },
      "source": [
        "ECONOMY_DOMAINS = ['finance', 'banking']\n",
        "\n",
        "def enrich_sentence(analysed_sentence, domains):\n",
        "\n",
        "    enriched_sentences = []\n",
        "\n",
        "    # For each token in the sentence\n",
        "    for token in sentence:\n",
        "      # get those synsets within the desired domains\n",
        "      synsets = token._.wordnet.wordnet_synsets_for_domain(domains)\n",
        "      if synsets:\n",
        "        lemmas_for_synset = []\n",
        "        for s in synsets:\n",
        "\n",
        "          # removing underscores between words so that have more words as sysnonyms\n",
        "          underscore_remover = [i.replace(\"_\", \" \") for i in s.lemma_names()]\n",
        "          \n",
        "          # If found a synset in the economy domains\n",
        "          # get the variants and add them to the enriched sentence\n",
        "          lemmas_for_synset.extend(underscore_remover)\n",
        "          enriched_sentences.append(' '.join(set(lemmas_for_synset)))\n",
        "      else:\n",
        "        enriched_sentences.append(token.text)\n",
        "\n",
        "    \n",
        "    #Please bear in mind that WordNet lemmas can be of multiple words, thus containing a \"_\" which we don't need.\n",
        "    \n",
        "    return enriched_sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb9WesUc-Q5c"
      },
      "source": [
        "#### Full search for intents\n",
        "\n",
        "Based on the `detect_intent` and `enrich_sentence` functions we set up the full logic that searches for intents.\n",
        "\n",
        "The function has to accept an analysed sentence (`Doc`), the list of intents and the list of domains as above, and then **try to find the intent in the default sentence. If not found, try to enrich the sentence, then search in the enriched ones. Return an intent if found.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-11T11:45:58.632646Z",
          "start_time": "2019-11-11T11:45:58.628020Z"
        },
        "id": "pOFJggjm-Q5d"
      },
      "source": [
        "def search_for_intents(analysed_sentence, intents, domains):\n",
        "\n",
        "    found_intent = None\n",
        "\n",
        "    # get the intent\n",
        "    found_intent = detect_intent(analysed_sentence, intents)\n",
        "\n",
        "    # enrich the sentence if no intent was found\n",
        "    if found_intent == None:\n",
        "      enriched_sent = enrich_sentence(analysed_sentence, domains)\n",
        "      # join the content of the list to make a sentence\n",
        "      sentence = nlp((' '.join(enriched_sent)))\n",
        "      #print(sentence) # see the sysnonyms to find patterns for each intent\n",
        "\n",
        "      #again search for the intent\n",
        "      found_intent = detect_intent(sentence, intents)\n",
        "      # even after having the sysnonyms, if no intent was found return none\n",
        "      # so that we can change the pattern\n",
        "      if found_intent == None:\n",
        "        pass\n",
        "      \n",
        "    return found_intent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO3muUJH-Q5i"
      },
      "source": [
        "#### Let's try this out!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbgNFF1D-t-y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07162dfe-7cd1-45e9-9245-baace37a667f"
      },
      "source": [
        "sentence = nlp(\"I would like to withdraw 5000 euros.\")\n",
        "\n",
        "found_intent = search_for_intents(sentence,INTENTS,ECONOMY_DOMAINS)\n",
        "\n",
        "print(found_intent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TAKEOUT_INTENT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIP616hO-Q5l"
      },
      "source": [
        "## Finally: parse the full query\n",
        "\n",
        "Refine the original patterns and all the functions until the tests pass at the end of the notebook. Use **the least amount of handmade patterns possible!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-11T11:45:58.700317Z",
          "start_time": "2019-11-11T11:45:58.696151Z"
        },
        "id": "4P-B93D1-Q5l"
      },
      "source": [
        "def parse_query(query, intents, domains, money):\n",
        "    \n",
        "    analysed_sentence = nlp(query)\n",
        "    found_intent = search_for_intents(analysed_sentence,intents, domains)\n",
        "    if found_intent == intents[0] or found_intent == intents[1]:\n",
        "        amount = detect_money(analysed_sentence,money)\n",
        "        if amount:\n",
        "            print(\"Executing\",found_intent,\"with\",amount)\n",
        "            return (found_intent, amount)\n",
        "        else:\n",
        "            print(\"No amount was given, please add one!\")\n",
        "            return (found_intent, None)\n",
        "    elif found_intent == intents[-1]:\n",
        "        print(\"Getting you your account balance, one moment...\")\n",
        "        return (found_intent, None)\n",
        "    else:\n",
        "        print(\"Can't parse what you are asking for, sorry!\")\n",
        "        return (None, None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-11T11:45:58.757534Z",
          "start_time": "2019-11-11T11:45:58.701701Z"
        },
        "id": "Qxtd0i9p-Q5p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b01e4b1-8a45-48bc-d20c-829c768b0597"
      },
      "source": [
        "parse_query(\"I would like to withdraw 5000.\",INTENTS, ECONOMY_DOMAINS, MONEY)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Executing TAKEOUT_INTENT with 5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('TAKEOUT_INTENT', '5000')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-11T11:45:58.763117Z",
          "start_time": "2019-11-11T11:45:58.758974Z"
        },
        "id": "t0UChqqw-Q5s"
      },
      "source": [
        "tests = [\n",
        "    (\"I would like to deposit 5000 euros.\",(\"PAYIN_INTENT\",\"5000\")),\n",
        "    (\"I would like to put in 5000 euros.\",(\"PAYIN_INTENT\",\"5000\")),\n",
        "    (\"I would like to pay in 5000 euros.\",(\"PAYIN_INTENT\",\"5000\")),\n",
        "    (\"I would like to pay up 5000EUR.\",(\"PAYIN_INTENT\",\"5000\")),\n",
        "    (\"Can I pay in 5000 euros, please?\",(\"PAYIN_INTENT\",\"5000\")),\n",
        "    \n",
        "    \n",
        "    (\"I would like to deposit money.\",(\"PAYIN_INTENT\",None)),\n",
        "    \n",
        "\n",
        "    (\"I am about to take out 5000 euros.\",(\"TAKEOUT_INTENT\",\"5000\")),\n",
        "    (\"I am about to get out 5000 euros.\",(\"TAKEOUT_INTENT\",\"5000\")),\n",
        "    (\"I am about to withdraw 5000 euros.\",(\"TAKEOUT_INTENT\",\"5000\")),\n",
        "    (\"I want to withdraw 5000 USD.\",(\"TAKEOUT_INTENT\",\"5000\")),\n",
        "    (\"Can I withdraw $5000.\",(\"TAKEOUT_INTENT\",\"5000\")),\n",
        "\n",
        "    \n",
        "    (\"Can I check my account, please?\",(\"BALANCE_INTENT\",None)),\n",
        "    (\"May I see my balance, please?\",(\"BALANCE_INTENT\",None)),\n",
        "    (\"Could I query my account, please?\",(\"BALANCE_INTENT\",None)),\n",
        "    (\"I would like to see my account balance.\",(\"BALANCE_INTENT\",None)),\n",
        "\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-11T11:45:59.303171Z",
          "start_time": "2019-11-11T11:45:58.764596Z"
        },
        "id": "JI0jLxmH-Q5y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac11bacc-2b28-400b-fee9-f512a7bf0564"
      },
      "source": [
        "for test in tests:\n",
        "    try:\n",
        "        assert parse_query(test[0],INTENTS, ECONOMY_DOMAINS, MONEY) == test[1]\n",
        "    except:\n",
        "        print(\"---ERROR: \",parse_query(test[0],INTENTS, ECONOMY_DOMAINS, MONEY))\n",
        "        raise"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Executing PAYIN_INTENT with 5000\n",
            "Executing PAYIN_INTENT with 5000\n",
            "Executing PAYIN_INTENT with 5000\n",
            "Executing PAYIN_INTENT with 5000\n",
            "Executing PAYIN_INTENT with 5000\n",
            "No amount was given, please add one!\n",
            "Executing TAKEOUT_INTENT with 5000\n",
            "Executing TAKEOUT_INTENT with 5000\n",
            "Executing TAKEOUT_INTENT with 5000\n",
            "Executing TAKEOUT_INTENT with 5000\n",
            "Executing TAKEOUT_INTENT with 5000\n",
            "Getting you your account balance, one moment...\n",
            "Getting you your account balance, one moment...\n",
            "Getting you your account balance, one moment...\n",
            "Getting you your account balance, one moment...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQw_G53D-Q51"
      },
      "source": [
        "A more elaborate and very nice example on the power of rule based matching and it's combination with machine learning models can be found [here](https://github.com/pmbaumgartner/binder-notebooks/blob/master/rule-based-matching-with-spacy-matcher.ipynb)"
      ]
    }
  ]
}